





import pickle

def load_batches(paths):
    all_cells = {}
    for path in paths:
        with open(path, 'rb') as f:
            batch = pickle.load(f)
        all_cells.update(batch)
    return all_cells

batch_paths = ['../data/batch1.pkl', '../data/batch2.pkl', '../data/batch3.pkl']
cells = load_batches(batch_paths)





cells['b1c0']['summary']








import numpy as np

def extract_features_simple(cell, max_cycle=100):

    # Var Q10, 100
    q_100 = cell['cycles']['100']['Qdlin']
    q_10 = cell['cycles']['10']['Qdlin']
    difference = q_100 - q_10
    var_q10_100 = np.var(difference)

    features = [
        np.log(var_q10_100),
    ]
    
    return features

def create_X_y_arrays(extract_features, cells):

    X_list = []
    y_list = []
    cell_ids = []
    
    for cid, cell in cells.items():
            
        features = extract_features(cell)
        X_list.append(features)
        y_list.append(cell['cycle_life'])
        cell_ids.append(cid)
    
    X_array = np.array(X_list)
    y_array = np.array(y_list).flatten()
    
    # Mask for valid entries
    mask = ~np.isnan(y_array)
    X = X_array[mask]
    y = y_array[mask]

    return X, y

X_simple, y = create_X_y_arrays(extract_features_simple, cells)





from sklearn.linear_model import ElasticNetCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.preprocessing import RobustScaler

def split_and_fit(X, y, model):

    # Split
    X_train, X_test, y_train, y_test = train_test_split(X, np.log(y), test_size=0.2, random_state=40)
    
    # Normalize based on training set only
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train model
    model.fit(X_train_scaled, y_train)
    
    # Predict
    y_pred_log = model.predict(X_test_scaled)
    y_pred = np.exp(y_pred_log)

    return y_pred

simple_model = ElasticNetCV(cv=5)
y_pred_simple = split_and_fit(X_simple, y, simple_model)





from sklearn.metrics import mean_squared_error

def evaluate(y_true, y_pred):
    
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    percent_errors = np.abs((y_true - y_pred) / y_true) * 100
    mpe = np.mean(percent_errors)
    print(f"Mean Percent Error: {mpe:.2f}%")
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    print(f"RMSE: {rmse:.2f}")
    print("RÂ²:", r2_score(y_true, y_pred))
    
    pass

# Evaluate
y_true = np.exp(y_test)
evaluate(y_true, y_pred_simple)











def extract_features_full(cell, max_cycle=100):
    summary = cell['summary']
    
    # Extract summary values
    qd = summary['QD']
    IR = summary['IR']
    charge_time = summary['chargetime']
    Tavg = summary['Tavg']
    Tmin = summary['Tmin']
    Tmax = summary['Tmax']
    
    valid_cycles = min(len(qd), max_cycle)
    
    # Convert to lists
    qd_vals = [qd[i] for i in range(valid_cycles)]
    IR_vals = [IR[i] for i in range(valid_cycles)]
    ct_vals = [charge_time[i] for i in range(valid_cycles)]
    Tavg_vals = [Tavg[i] for i in range(valid_cycles)]
    Tmin_vals = [Tmin[i] for i in range(valid_cycles)]
    Tmax_vals = [Tmax[i] for i in range(valid_cycles)]

    # Discharge capacity features
    qd_mean = np.mean(qd_vals)
    qd_var = np.var(qd_vals)

    # Internal resistance features
    IR_mean = np.mean(IR_vals)
    IR_std = np.std(IR_vals)

    # Charge time features
    ct_mean = np.mean(ct_vals)
    ct_std = np.std(ct_vals)

    # Temperature features
    Tavg_mean = np.mean(Tavg_vals)
    Tmin_mean = np.mean(Tmin_vals)
    Tmax_mean = np.mean(Tmax_vals)
    Tavg_std = np.std(Tavg_vals)
    Tmin_std = np.std(Tmin_vals)
    Tmax_std = np.std(Tmax_vals)

    # Var Q10, 100
    q_100 = cell['cycles']['100']['Qdlin']
    q_10 = cell['cycles']['10']['Qdlin']
    difference = q_100 - q_10
    min_q10_100 = min(difference)
    mean_q10_100 = np.mean(difference)
    var_q10_100 = np.var(difference)

    features = [
        qd_mean, qd_var,
        IR_mean, IR_std,
        ct_mean, ct_std,
        Tavg_mean, Tmin_mean, Tmax_mean,
        Tavg_std, Tmin_std, Tmax_std,
        min_q10_100, mean_q10_100, np.log(var_q10_100)
    ]
    
    return features

X_full, y = create_X_y_arrays(extract_features_full, cells)
model_full = ElasticNetCV(cv=5)
y_pred_full = split_and_fit(X_full, y, model_full)
evaluate(y_true, y_pred_full)








import matplotlib.pyplot as plt

plt.figure(figsize=(6, 6))
plt.scatter(np.exp(y_train), y_pred_train, alpha=0.7, label='train')
plt.scatter(y_true, y_pred_simple, alpha=0.7, label='simple model predictions')
plt.scatter(y_true, y_pred_full, alpha=0.7, label='full model predictions')
plt.plot([min(np.exp(y_train)), max(np.exp(y_train))], [min(np.exp(y_train)), max(np.exp(y_train))], 'r--', label='Ideal')

plt.xlabel("True Cycle Life")
plt.ylabel("Predicted Cycle Life")
plt.title("Model Prediction vs. True Values")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()





import seaborn as sns
import pandas as pd

feature_names = [
    "qd_mean", "qd_var", 
    "IR_mean", "IR_std", 
    "ct_mean", "ct_std", 
    "Tavg_mean", "Tmin_mean", "Tmax_mean", 
    "Tavg_std", "Tmin_std", "Tmax_std", 
    "min_q10_100", "mean_q10_100", "log_var_q10_100"
]

coefs = model_full.coef_
importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": coefs,
    "Abs Coefficient": np.abs(coefs)
}).sort_values("Abs Coefficient", ascending=False)

plt.figure(figsize=(10, 6))
sns.heatmap(importance_df.set_index("Feature")[["Abs Coefficient"]].T, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("ElasticNet Feature Importance (absolute coefficients)")
plt.tight_layout()
plt.show()







